{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bba59f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "PATH = r\".\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2226a787",
   "metadata": {},
   "source": [
    "# File Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb6aa2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = os.path.join(PATH,'Images')\n",
    "frames = os.listdir(file_path)\n",
    "func = lambda f : int(f.split('.')[0])\n",
    "frames = sorted(frames, key=func)\n",
    "frames = list(map(lambda x : os.path.join(file_path,x),frames))\n",
    "frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca816588",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiveGraphPlot:\n",
    "    def __init__(self, graph_width=800, graph_height=400, num_points=100):\n",
    "        self.graph_width = graph_width\n",
    "        self.graph_height = graph_height\n",
    "        self.num_points = num_points\n",
    "        self.data = []\n",
    "        self.data2 = []\n",
    "\n",
    "    def plot_graph(self, value1,value2):\n",
    "        self.data.append(value1)\n",
    "        self.data = self.data[-self.num_points:]  # Keep only the latest data points\n",
    "\n",
    "        self.data2.append(value2)\n",
    "        self.data2 = self.data2[-self.num_points:]  # Keep only the latest data points\n",
    "        \n",
    "        plt.clf()  # Clear the figure\n",
    "        plt.plot(range(len(self.data)), self.data)\n",
    "        plt.plot(range(len(self.data2)), self.data2)\n",
    "        plt.xlabel('Time')\n",
    "        plt.ylabel('Value')\n",
    "        plt.title('Live Graph')\n",
    "        plt.grid(False)\n",
    "\n",
    "        # Convert the plot to an image\n",
    "        fig = plt.gcf()\n",
    "        fig.canvas.draw()\n",
    "        plot_img = np.array(fig.canvas.renderer.buffer_rgba())\n",
    "            \n",
    "        # Convert RGBA to RGB\n",
    "        plot_img = cv2.cvtColor(plot_img, cv2.COLOR_RGBA2RGB)\n",
    "\n",
    "        # Resize the plot image to fit the specified graph width and height\n",
    "        plot_img = cv2.resize(plot_img, (self.graph_width, self.graph_height))\n",
    "\n",
    "        return plot_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4424ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in write mode\n",
    "file_path = os.path.join(PATH,'turn_dir.csv')\n",
    "data = pd.read_csv(file_path)\n",
    "data.drop(data.columns[[0,1]],axis=1,inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05aef03",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['steer'].plot()\n",
    "plt.vlines(1450,-1,1,colors='g')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b288c8d0",
   "metadata": {},
   "source": [
    "# Dataloading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe8fff1",
   "metadata": {},
   "source": [
    "With Turning and Traffic:\n",
    "\n",
    "    Left : 1472\n",
    "    Right : 775\n",
    "    Straight : 721\n",
    "    \n",
    "With Turning and not Traffic:\n",
    "\n",
    "    Left : 178\n",
    "    Right : 372\n",
    "    Straight : 0\n",
    "    \n",
    "Without Turning and Traffic:\n",
    "\n",
    "    Length : 4693"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deafab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_majority = np.array(data[(data[\"Turn_signal\"] == \"Straight\")].steer)\n",
    "plt.plot(df_majority)\n",
    "plt.show()\n",
    "print(df_majority.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffba0f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self,mode = 'train',over_sampling = True):\n",
    "        #len_frames = len(frames)\n",
    "        #n = int(0.8*len_frames)\n",
    "        \n",
    "        self.img_dir_path = os.path.join(PATH,'Images')\n",
    "        if mode == 'train':\n",
    "            self.data = data.iloc[:8000]\n",
    "            \n",
    "            #Over-sampling\n",
    "            if over_sampling == True:\n",
    "                \n",
    "                Left_Signal_population = data[(data[\"Turn_signal\"] == \"Left\")]\n",
    "                Right_Signal_population = data[(data[\"Turn_signal\"] == \"Right\")]\n",
    "                Straight_Signal_population = data[(data[\"Turn_signal\"] == \"Straight\")]\n",
    "                \n",
    "                resample_length =  max(data[(data[\"Turn_signal\"] == \"Left\")].shape[0],\n",
    "                                       data[(data[\"Turn_signal\"] == \"Right\")].shape[0],\n",
    "                                       data[(data[\"Turn_signal\"] == \"Straight\")].shape[0])\n",
    "                \n",
    "                Left_Signal_population_oversampled = resample(Left_Signal_population, n_samples=resample_length, replace=True, random_state=42)\n",
    "                Right_Signal_population_oversampled = resample(Right_Signal_population, n_samples=resample_length, replace=True, random_state=42)\n",
    "                Straight_Signal_population_oversampled = resample(Straight_Signal_population, n_samples=resample_length, replace=True, random_state=42)\n",
    "\n",
    "                self.data = pd.concat([Left_Signal_population_oversampled, \n",
    "                                       Right_Signal_population_oversampled,\n",
    "                                       Straight_Signal_population_oversampled])\n",
    "                #print(self.data.shape)\n",
    "                #print(Left_Signal_population_oversampled.shape)\n",
    "                #print(Right_Signal_population_oversampled.shape)\n",
    "                #print(Straight_Signal_population_oversampled.shape)\n",
    "                \n",
    "        else:\n",
    "            self.data = data.iloc[8000:]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Implement your logic to retrieve and preprocess the data\n",
    "        sample = self.data.iloc[index]['pic_file_name']\n",
    "        sample = os.path.join(self.img_dir_path,sample)\n",
    "        img = cv2.imread(sample)\n",
    "        #print(img.shape)\n",
    "        #img = transform(img).numpy()\n",
    "        img = cv2.resize(img, (224,224))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "        \n",
    "        velocity = self.data.iloc[index]['velocity']\n",
    "        \n",
    "        if self.data.iloc[index][\"Turn_signal\"] == \"Left\":\n",
    "            command = torch.tensor([1,0,0])\n",
    "        elif self.data.iloc[index][\"Turn_signal\"] == \"Right\":\n",
    "            command = torch.tensor([0,0,1])\n",
    "        elif self.data.iloc[index][\"Turn_signal\"] == \"Straight\":\n",
    "            command = torch.tensor([0,1,0]) \n",
    "        else:\n",
    "            command = torch.tensor([0,0,0])\n",
    "        \n",
    "        throttle = self.data.iloc[index]['throttle']\n",
    "        steer = self.data.iloc[index]['steer']\n",
    "        brake = self.data.iloc[index]['brake']\n",
    "        \n",
    "        # Return the sample as a tuple or dictionary\n",
    "        return (img,velocity,command),(steer,throttle,brake)\n",
    "\n",
    "# Create an instance of your custom dataset\n",
    "train_dataset = CustomDataset('train')\n",
    "val_dataset = CustomDataset('val')\n",
    "\n",
    "# Create a data loader\n",
    "batch_size = 16\n",
    "dataloader = {'train' : DataLoader(train_dataset, batch_size=batch_size, shuffle=True),\n",
    "              'val' : DataLoader(val_dataset, batch_size=batch_size, shuffle=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed48ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index,i in enumerate(dataloader['train']):\n",
    "    print(i[0][0].shape,i[1][0].shape)\n",
    "    if index == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42492c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = i[0][0]\n",
    "speed = i[0][1]\n",
    "cmd = i[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8012c38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(i[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b883a5",
   "metadata": {},
   "source": [
    "for j in range(200):\n",
    "    cv2.imshow(\"Frame\",i[0][0][j].cpu().numpy().astype('uint8'))\n",
    "        \n",
    "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac8544a",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5ebdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class NewGELUActivation(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of the GELU activation function currently in Google BERT repo (identical to OpenAI GPT). Also see\n",
    "    the Gaussian Error Linear Units paper: https://arxiv.org/abs/1606.08415\n",
    "\n",
    "    Taken from https://github.com/huggingface/transformers/blob/main/src/transformers/activations.py\n",
    "    \"\"\"\n",
    "\n",
    "    def forward(self, input):\n",
    "        return 0.5 * input * (1.0 + torch.tanh(math.sqrt(2.0 / math.pi) * (input + 0.044715 * torch.pow(input, 3.0))))\n",
    "\n",
    "\n",
    "class PatchEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Convert the image into patches and then project them into a vector space.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.image_size = config[\"image_size\"]\n",
    "        self.patch_size = config[\"patch_size\"]\n",
    "        self.num_channels = config[\"num_channels\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        # Calculate the number of patches from the image size and patch size\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "        # Create a projection layer to convert the image into patches\n",
    "        # The layer projects each patch into a vector of size hidden_size\n",
    "        self.projection = nn.Conv2d(self.num_channels, self.hidden_size, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, num_channels, image_size, image_size) -> (batch_size, num_patches, hidden_size)\n",
    "        x = self.projection(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Combine the patch embeddings with the class token and position embeddings.\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.patch_embeddings = PatchEmbeddings(config)\n",
    "        # Create a learnable [CLS] token\n",
    "        # Similar to BERT, the [CLS] token is added to the beginning of the input sequence\n",
    "        # and is used to classify the entire sequence\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, config[\"hidden_size\"]))\n",
    "        # Create position embeddings for the [CLS] token and the patch embeddings\n",
    "        # Add 1 to the sequence length for the [CLS] token\n",
    "        self.position_embeddings = \\\n",
    "            nn.Parameter(torch.randn(1, self.patch_embeddings.num_patches + 1, config[\"hidden_size\"]))\n",
    "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embeddings(x)\n",
    "        batch_size, _, _ = x.size()\n",
    "        # Expand the [CLS] token to the batch size\n",
    "        # (1, 1, hidden_size) -> (batch_size, 1, hidden_size)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        # Concatenate the [CLS] token to the beginning of the input sequence\n",
    "        # This results in a sequence length of (num_patches + 1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        # print(\"X shape : \",x.shape,\"Positional Embeddding : \",self.position_embeddings.shape)\n",
    "        x = x + self.position_embeddings\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    A single attention head.\n",
    "    This module is used in the MultiHeadAttention module.\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_size, attention_head_size, dropout, bias=True):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_head_size = attention_head_size\n",
    "        # Create the query, key, and value projection layers\n",
    "        self.query = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.key = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "        self.value = nn.Linear(hidden_size, attention_head_size, bias=bias)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Project the input into query, key, and value\n",
    "        # The same input is used to generate the query, key, and value,\n",
    "        # so it's usually called self-attention.\n",
    "        # (batch_size, sequence_length, hidden_size) -> (batch_size, sequence_length, attention_head_size)\n",
    "        query = self.query(x)\n",
    "        key = self.key(x)\n",
    "        value = self.value(x)\n",
    "        # Calculate the attention scores\n",
    "        # softmax(Q*K.T/sqrt(head_size))*V\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "        # Calculate the attention output\n",
    "        attention_output = torch.matmul(attention_probs, value)\n",
    "        return (attention_output, attention_probs)\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention module.\n",
    "    This module is used in the TransformerEncoder module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
    "        # The attention head size is the hidden size divided by the number of attention heads\n",
    "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        # Whether or not to use bias in the query, key, and value projection layers\n",
    "        self.qkv_bias = config[\"qkv_bias\"]\n",
    "        # Create a list of attention heads\n",
    "        self.heads = nn.ModuleList([])\n",
    "        for _ in range(self.num_attention_heads):\n",
    "            head = AttentionHead(\n",
    "                self.hidden_size,\n",
    "                self.attention_head_size,\n",
    "                config[\"attention_probs_dropout_prob\"],\n",
    "                self.qkv_bias\n",
    "            )\n",
    "            self.heads.append(head)\n",
    "        # Create a linear layer to project the attention output back to the hidden size\n",
    "        # In most cases, all_head_size and hidden_size are the same\n",
    "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
    "        self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Calculate the attention output for each attention head\n",
    "        attention_outputs = [head(x) for head in self.heads]\n",
    "        # Concatenate the attention outputs from each attention head\n",
    "        attention_output = torch.cat([attention_output for attention_output, _ in attention_outputs], dim=-1)\n",
    "        # Project the concatenated attention output back to the hidden size\n",
    "        attention_output = self.output_projection(attention_output)\n",
    "        attention_output = self.output_dropout(attention_output)\n",
    "        # Return the attention output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (attention_output, None)\n",
    "        else:\n",
    "            attention_probs = torch.stack([attention_probs for _, attention_probs in attention_outputs], dim=1)\n",
    "            return (attention_output, attention_probs)\n",
    "\n",
    "\n",
    "class FasterMultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention module with some optimizations.\n",
    "    All the heads are processed simultaneously with merged query, key, and value projections.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
    "        # The attention head size is the hidden size divided by the number of attention heads\n",
    "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        # Whether or not to use bias in the query, key, and value projection layers\n",
    "        self.qkv_bias = config[\"qkv_bias\"]\n",
    "        # Create a linear layer to project the query, key, and value\n",
    "        self.qkv_projection = nn.Linear(self.hidden_size, self.all_head_size * 3, bias=self.qkv_bias)\n",
    "        self.attn_dropout = nn.Dropout(config[\"attention_probs_dropout_prob\"])\n",
    "        # Create a linear layer to project the attention output back to the hidden size\n",
    "        # In most cases, all_head_size and hidden_size are the same\n",
    "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
    "        self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Project the query, key, and value\n",
    "        # (batch_size, sequence_length, hidden_size) -> (batch_size, sequence_length, all_head_size * 3)\n",
    "        qkv = self.qkv_projection(x)\n",
    "        # Split the projected query, key, and value into query, key, and value\n",
    "        # (batch_size, sequence_length, all_head_size * 3) -> (batch_size, sequence_length, all_head_size)\n",
    "        query, key, value = torch.chunk(qkv, 3, dim=-1)\n",
    "        # Resize the query, key, and value to (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
    "        batch_size, sequence_length, _ = query.size()\n",
    "        query = query.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
    "        key = key.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
    "        value = value.view(batch_size, sequence_length, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n",
    "        # Calculate the attention scores\n",
    "        # softmax(Q*K.T/sqrt(head_size))*V\n",
    "        attention_scores = torch.matmul(query, key.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n",
    "        attention_probs = self.attn_dropout(attention_probs)\n",
    "        # Calculate the attention output\n",
    "        attention_output = torch.matmul(attention_probs, value)\n",
    "        # Resize the attention output\n",
    "        # from (batch_size, num_attention_heads, sequence_length, attention_head_size)\n",
    "        # To (batch_size, sequence_length, all_head_size)\n",
    "        attention_output = attention_output.transpose(1, 2) \\\n",
    "                                           .contiguous() \\\n",
    "                                           .view(batch_size, sequence_length, self.all_head_size)\n",
    "        # Project the attention output back to the hidden size\n",
    "        attention_output = self.output_projection(attention_output)\n",
    "        attention_output = self.output_dropout(attention_output)\n",
    "        # Return the attention output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (attention_output, None)\n",
    "        else:\n",
    "            return (attention_output, attention_probs)\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    A multi-layer perceptron module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense_1 = nn.Linear(config[\"hidden_size\"], config[\"intermediate_size\"])\n",
    "        self.activation = NewGELUActivation()\n",
    "        self.dense_2 = nn.Linear(config[\"intermediate_size\"], config[\"hidden_size\"])\n",
    "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    A single transformer block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.use_faster_attention = config.get(\"use_faster_attention\", False)\n",
    "        if self.use_faster_attention:\n",
    "            self.attention = FasterMultiHeadAttention(config)\n",
    "        else:\n",
    "            self.attention = MultiHeadAttention(config)\n",
    "        self.layernorm_1 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "        self.mlp = MLP(config)\n",
    "        self.layernorm_2 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Self-attention\n",
    "        attention_output, attention_probs = \\\n",
    "            self.attention(self.layernorm_1(x), output_attentions=output_attentions)\n",
    "        # Skip connection\n",
    "        x = x + attention_output\n",
    "        # Feed-forward network\n",
    "        mlp_output = self.mlp(self.layernorm_2(x))\n",
    "        # Skip connection\n",
    "        x = x + mlp_output\n",
    "        # Return the transformer block's output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (x, None)\n",
    "        else:\n",
    "            return (x, attention_probs)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The transformer encoder module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        # Create a list of transformer blocks\n",
    "        self.blocks = nn.ModuleList([])\n",
    "        for _ in range(config[\"num_hidden_layers\"]):\n",
    "            block = Block(config)\n",
    "            self.blocks.append(block)\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Calculate the transformer block's output for each block\n",
    "        all_attentions = []\n",
    "        for block in self.blocks:\n",
    "            x, attention_probs = block(x, output_attentions=output_attentions)\n",
    "            if output_attentions:\n",
    "                all_attentions.append(attention_probs)\n",
    "        # Return the encoder's output and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (x, None)\n",
    "        else:\n",
    "            return (x, all_attentions)\n",
    "    \n",
    "class ViT(nn.Module):\n",
    "    \"\"\"\n",
    "    The ViT model for classification.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.image_size = config[\"image_size\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_classes = config[\"num_classes\"]\n",
    "        # Create the embedding module\n",
    "        self.embedding = Embeddings(config)\n",
    "        # Create the transformer encoder module\n",
    "        self.encoder = Encoder(config)\n",
    "        # Create a linear layer to project the encoder's output to the number of classes\n",
    "        # self.classifier = nn.Linear(self.hidden_size, self.num_classes)\n",
    "        # self.encoder_fc = nn.Linear(self.hidden_size, self.num_classes)\n",
    "        # Initialize the weights\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward(self, x, output_attentions=False):\n",
    "        # Calculate the embedding output\n",
    "        embedding_output = self.embedding(x)\n",
    "        # Calculate the encoder's output\n",
    "        encoder_output, all_attentions = self.encoder(embedding_output, output_attentions=output_attentions)\n",
    "        # Calculate the logits, take the [CLS] token's output as features for classification\n",
    "        # logits = self.classifier(encoder_output[:, 0, :])\n",
    "        \n",
    "        # Return the logits and the attention probabilities (optional)\n",
    "        if not output_attentions:\n",
    "            return (encoder_output,None)\n",
    "            #return (logits, None)\n",
    "        else:\n",
    "            return (encoder_output,all_attentions)\n",
    "            #return (logits, all_attentions)\n",
    "    \n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Conv2d)):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=self.config[\"initializer_range\"])\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "        elif isinstance(module, Embeddings):\n",
    "            module.position_embeddings.data = nn.init.trunc_normal_(\n",
    "                module.position_embeddings.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=self.config[\"initializer_range\"],\n",
    "            ).to(module.position_embeddings.dtype)\n",
    "\n",
    "            module.cls_token.data = nn.init.trunc_normal_(\n",
    "                module.cls_token.data.to(torch.float32),\n",
    "                mean=0.0,\n",
    "                std=self.config[\"initializer_range\"],\n",
    "            ).to(module.cls_token.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b61c794",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fc_block(nn.Module):\n",
    "    def __init__(self,in_channel,out_channel,dropout,act = True):\n",
    "        super(fc_block, self).__init__()\n",
    "        self.fc = nn.Linear(in_channel, out_channel)\n",
    "        self.bn = nn.BatchNorm1d(out_channel)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.act = act\n",
    "\n",
    "    def forward(self,x):\n",
    "        #print(x.shape)\n",
    "        x = self.fc(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.dropout(x)\n",
    "        if self.act == True:\n",
    "            x = self.relu(x)\n",
    "        return x\n",
    "    \n",
    "class branch(nn.Module):\n",
    "    def __init__(self,in_channel,out_channel,dropout):\n",
    "      super(branch,self).__init__()\n",
    "      self.fc1 = fc_block( in_channel, 256, 0.2)\n",
    "      self.fc2 = fc_block( 256, 256, 0.2)\n",
    "      self.fc3 = fc_block( 256, out_channel, 0.2,act = False)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "class ViT_backend(nn.Module):\n",
    "    def __init__(self,config):\n",
    "        super(ViT_backend,self).__init__()\n",
    "        \n",
    "        self.speed_fc = fc_block(1, config[\"speed_hidden_size\"],0.2)\n",
    "        self.cmd_fc = fc_block(3, config[\"cmd_hidden_size\"],0.2)\n",
    "        self.vit = ViT(config)\n",
    "        \n",
    "        self.branch = branch(config[\"speed_hidden_size\"]+config[\"cmd_hidden_size\"]+config[\"hidden_size\"],3,0.2)\n",
    "        \n",
    "    def forward(self,x_img,x_speed,commands):\n",
    "               \n",
    "        x_img,attention  = self.vit(x_img,True)\n",
    "        x_img = x_img[:,0,:]\n",
    "        x_speed = self.speed_fc(x_speed)\n",
    "        x_cmd = self.cmd_fc(commands)\n",
    "\n",
    "        # Fusion \n",
    "        # print(x_img.shape,x_speed.shape,x_cmd.shape)\n",
    "        x_joint = torch.cat([x_img,x_speed,x_cmd],axis = 1)\n",
    "        return self.branch(x_joint),attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253f4f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"patch_size\": 8,  # Input image size: 224x224 -> 4x4 patches\n",
    "    \"hidden_size\": 32,\n",
    "    \"num_hidden_layers\": 1,\n",
    "    \"num_attention_heads\": 2,\n",
    "    \"intermediate_size\": 4 * 32, # 4 * hidden_size\n",
    "    \"hidden_dropout_prob\": 0.0,\n",
    "    \"attention_probs_dropout_prob\": 0.0,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"image_size\": 224,\n",
    "    \"num_classes\": 3, # num_classes of CIFAR10\n",
    "    \"num_channels\": 3,\n",
    "    \"qkv_bias\": True,\n",
    "    \"use_faster_attention\": True,\n",
    "    \n",
    "    \"speed_hidden_size\": 12,\n",
    "    \"cmd_hidden_size\": 10,\n",
    "}\n",
    "# These are not hard constraints, but are used to prevent misconfigurations\n",
    "assert config[\"hidden_size\"] % config[\"num_attention_heads\"] == 0\n",
    "assert config['intermediate_size'] == 4 * config['hidden_size']\n",
    "assert config['image_size'] % config['patch_size'] == 0\n",
    "\n",
    "model = ViT_backend(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ace0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of parameters\n",
    "num_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(\"Number of parameters in the model:\", num_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f68c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5712e0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "out,attention_maps = model(i[0][0].permute(0,3,1,2).to(torch.float32),torch.randn(16,1),torch.randn(16,3))\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc085039",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_maps[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0122c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "\n",
    "attention_maps = torch.cat(attention_maps, dim=1)\n",
    "print(attention_maps.shape)\n",
    "attention_maps = attention_maps[:, :, 0, 1:]\n",
    "print(attention_maps.shape)\n",
    "attention_maps = attention_maps.mean(dim=1)\n",
    "num_patches = attention_maps.size(-1)\n",
    "size = int(math.sqrt(num_patches))\n",
    "attention_maps = attention_maps.view(-1, size, size)\n",
    "print(attention_maps.shape)\n",
    "attention_maps = attention_maps.unsqueeze(1)\n",
    "attention_maps = F.interpolate(attention_maps, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "attention_maps = attention_maps.squeeze(1)\n",
    "print(attention_maps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a361dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "i[0][0].permute(0,3,1,2).to(torch.uint8).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b1c788",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the images and the attention maps\n",
    "num_images = 16\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "mask = np.concatenate([np.ones((224, 224)), np.zeros((224, 224))], axis=1)\n",
    "print(mask.shape)\n",
    "for i in range(num_images):\n",
    "    ax = fig.add_subplot(6, 5, i+1, xticks=[], yticks=[])\n",
    "    img = np.concatenate((img[i], img[i]), axis=1)\n",
    "    ax.imshow(img)\n",
    "    # Mask out the attention map of the left image\n",
    "    extended_attention_map = np.concatenate((np.zeros((224, 224)), attention_maps[i].detach().numpy()), axis=1)\n",
    "    extended_attention_map = np.ma.masked_where(mask==1, extended_attention_map)\n",
    "    ax.imshow(extended_attention_map, alpha=0.5, cmap='jet')\n",
    "    # Show the ground truth and the prediction\n",
    "    # gt = classes[labels[i]]\n",
    "    # pred = classes[predictions[i]]\n",
    "    #ax.set_title(f\"gt: {gt} / pred: {pred}\", color=(\"green\" if gt==pred else \"red\"))\n",
    "    #if output is not None:\n",
    "    #   plt.savefig(output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a1582b",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13237eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = 'cpu'\n",
    "\n",
    "# Train your model\n",
    "model = ViT_backend(config).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cb6540",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define your loss function and optimizer\n",
    "from livelossplot import PlotLosses\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01,weight_decay=1e-2)\n",
    "FILE_PATH = \"model_vit.pth\"\n",
    "model.load_state_dict(torch.load(FILE_PATH))\n",
    "\n",
    "liveloss = PlotLosses()\n",
    "\n",
    "for epoch in tqdm(range(50),desc = 'Epochs : '):\n",
    "    logs = {}\n",
    "    \n",
    "    for phase in ['train','val']:\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for inputs,targets in tqdm(dataloader[phase],desc = phase + ' : '):\n",
    "\n",
    "            inputs_img = inputs[0].permute(0,3,1,2).to(device).to(torch.float32)\n",
    "            inputs_speed = inputs[1].unsqueeze(1).to(device).to(torch.float32)\n",
    "            inputs_command = inputs[2].to(device).to(torch.float32)\n",
    "            labels = torch.cat([i.unsqueeze(1) for i in targets],axis = 1).to(device).to(torch.float32)\n",
    "\n",
    "            outputs,attentions = model(inputs_img,inputs_speed,inputs_command)\n",
    "\n",
    "            if phase == 'train':\n",
    "                optimizer.zero_grad()\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * inputs_img.size(0)\n",
    "            epoch_loss = running_loss / len(dataloader[phase].dataset)\n",
    "\n",
    "            prefix = ''\n",
    "            if phase == 'val':\n",
    "                prefix = 'val_'\n",
    "\n",
    "            logs[prefix + 'mse'] = np.sqrt(epoch_loss)\n",
    "\n",
    "            # Save the model\n",
    "            torch.save(model.state_dict(), FILE_PATH)\n",
    "\n",
    "    liveloss.update(logs)\n",
    "    liveloss.draw()\n",
    "\n",
    "\n",
    "    print(\"Epoch {} loss: {}\".format(epoch, epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82637e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of your custom dataset\n",
    "train_dataset = CustomDataset(mode = 'train',over_sampling = False)\n",
    "val_dataset = CustomDataset(mode = 'val',over_sampling = False)\n",
    "\n",
    "dataloader = {'train' : DataLoader(train_dataset, batch_size=batch_size, shuffle=False),\n",
    "              'val' : DataLoader(val_dataset, batch_size=batch_size, shuffle=False)}\n",
    "\n",
    "in_data1 = []\n",
    "in_data2 = []\n",
    "out_data = []\n",
    "for idx,(inputs,outputs) in enumerate(dataloader['val']):\n",
    "    in_data1.append(inputs[0])\n",
    "    in_data2.append(inputs[1])\n",
    "    out_data.append(torch.cat([i.unsqueeze(1) for i in outputs],axis = 1))\n",
    "    \n",
    "in_data1 = torch.cat(in_data1,axis = 0).to(device).permute(0,3,1,2)\n",
    "in_data2 = torch.cat(in_data2,axis = 0).to(device).unsqueeze(1).to(torch.float32)\n",
    "out_data = torch.cat(out_data,axis = 0).to(device)\n",
    "\n",
    "in_data1.shape,in_data2.shape,out_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa0693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_data = model(in_data1,in_data2,inputs_command)\n",
    "\n",
    "out_data = out_data.detach().cpu().numpy()\n",
    "pred_data = pred_data.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605814aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_smoothing(arr, alpha = 0.2):\n",
    "    smoothed = np.zeros_like(arr)\n",
    "    smoothed[0] = arr[0]  # The first value remains the same\n",
    "\n",
    "    for i in range(1, len(arr)):\n",
    "        smoothed[i] = alpha * arr[i] + (1 - alpha) * smoothed[i - 1]\n",
    "\n",
    "    return smoothed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35fbae5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8,5))\n",
    "plt.plot(pred_data[:,0])\n",
    "plt.plot(out_data[:,0])\n",
    "plt.plot(exponential_smoothing(pred_data[:,0]))\n",
    "plt.legend([\"pred\",\"Actual\",\"smoothed\"])\n",
    "plt.title(\"Steer\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.plot(pred_data[:,1])\n",
    "plt.plot(out_data[:,1])\n",
    "plt.plot(exponential_smoothing(pred_data[:,1]))\n",
    "plt.legend([\"pred\",\"Actual\",\"smoothed\"])\n",
    "plt.title(\"Acc Pedal\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.plot(pred_data[:,2])\n",
    "plt.plot(out_data[:,2])\n",
    "plt.plot(exponential_smoothing(pred_data[:,2]))\n",
    "plt.legend([\"pred\",\"Actual\",\"smoothed\"])\n",
    "plt.title(\"Brake\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e64f5d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
